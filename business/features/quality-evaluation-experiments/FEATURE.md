# Quality Evaluation + Experiments

## What it is
Run structured evaluations (offline and/or in production): compare prompts/models, detect regressions, and score outputs for quality.

## Why it matters
Latency and errors are only half the story; quality failures (hallucinations, bias, toxicity) impact users and trust.

## Key signals
- Evaluation scores (task-specific + general)
- Experiment metadata (prompt/model variants)
- Human feedback / labels
- Guardrail outcomes and failure reasons

## Noted in this landscape
Datadog (structured experiments + quality eval), New Relic (quality assurance), Traceloop (automated quality evaluations).
